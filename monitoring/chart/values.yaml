# Global settings
global:
  namespace: monitoring

# Prometheus configuration
prometheus:
  enabled: true
  alertmanager:
    enabled: false
  
  prometheus-pushgateway:
    enabled: false
  
  kube-state-metrics:
    enabled: true
  
  prometheus-node-exporter:
    enabled: true
  
  server:
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
    
    retention: "7d"
    
    extraArgs:
      web.enable-remote-write-receiver: ""
    
    service:
      type: ClusterIP
      servicePort: 80
    
    persistentVolume:
      enabled: false
    
    emptyDir:
      sizeLimit: ""
    
    serverFiles:
      prometheus.yml:
        scrape_configs:
          - job_name: 'otel-collector'
            static_configs:
              - targets: ['monitoring-stack--opentelemetry-collector.monitoring.svc.cluster.local:8888']
          
          - job_name: 'backend'
            kubernetes_sd_configs:
              - role: pod
            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_label_app]
                regex: diiage-finals-app.*backend
                action: keep
              - source_labels: [__meta_kubernetes_pod_ip]
                target_label: __address__
                action: replace
                regex: '(.+)'
                replacement: '$$1:8080'
              - source_labels: [__meta_kubernetes_pod_name]
                target_label: pod_name

# Grafana configuration
grafana:
  enabled: true
  adminUser: admin
  adminPassword: admin
  
  service:
    type: ClusterIP
    port: 3000
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  grafana.ini:
    auth.anonymous:
      enabled: true
      org_role: Admin
  
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          access: proxy
          url: http://monitoring-stack-prometheus-server.monitoring.svc.cluster.local:80
          isDefault: true
          editable: true
          jsonData:
            timeInterval: 15s
        
        - name: Tempo
          type: tempo
          access: proxy
          url: http://monitoring-stack-tempo.monitoring.svc.cluster.local:3200
          editable: true
          jsonData:
            tracesToLogs:
              datasourceUid: Loki
            serviceMap:
              datasourceUid: Prometheus
            nodeGraph:
              enabled: true
  
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  
  dashboards:
    default:
      backend-metrics:
        json: |
          {
            "title": "Backend Metrics",
            "tags": ["backend", "prometheus"],
            "timezone": "browser",
            "schemaVersion": 16,
            "version": 0,
            "refresh": "10s",
            "panels": [
              {
                "title": "HTTP Requests Total",
                "type": "graph",
                "gridPos": {"x": 0, "y": 0, "w": 12, "h": 8},
                "targets": [
                  {
                    "expr": "sum(rate(http_requests_total[5m])) by (path, status)",
                    "legendFormat": "{{path}} - {{status}}"
                  }
                ]
              },
              {
                "title": "ConfigMap Reads",
                "type": "graph",
                "gridPos": {"x": 12, "y": 0, "w": 12, "h": 8},
                "targets": [
                  {
                    "expr": "rate(configmap_read_total[5m])",
                    "legendFormat": "ConfigMap Reads/sec"
                  }
                ]
              }
            ]
          }
  
  persistence:
    enabled: false

# Tempo configuration
tempo:
  enabled: true
  replicas: 1
  
  tempo:
    repository: grafana/tempo
    tag: 2.3.1
    pullPolicy: IfNotPresent
    
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 1Gi
    
    storage:
      trace:
        backend: local
        local:
          path: /tmp/tempo/traces
        wal:
          path: /tmp/tempo/wal
    
    ingester:
      max_block_duration: 5m
    
    compactor:
      compaction:
        block_retention: 48h
    
    server:
      http_listen_port: 3200
    
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
  
  persistence:
    enabled: false
  
  securityContext:
    runAsUser: 10001
    runAsGroup: 10001
    fsGroup: 10001
    runAsNonRoot: true

# OpenTelemetry Collector configuration
opentelemetry-collector:
  enabled: true
  mode: deployment
  
  image:
    repository: otel/opentelemetry-collector-contrib
  
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
  
  replicaCount: 1
  
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    prometheus:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    metrics:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      protocol: TCP
  
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      
      prometheus:
        config:
          scrape_configs:
            - job_name: 'otel-collector'
              scrape_interval: 10s
              static_configs:
                - targets: ['localhost:8888']
    
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
      
      memory_limiter:
        check_interval: 1s
        limit_mib: 512
        spike_limit_mib: 128
    
    exporters:
      otlp/tempo:
        endpoint: monitoring-stack-tempo.monitoring.svc.cluster.local:4317
        tls:
          insecure: true

      prometheusremotewrite:
        endpoint: http://monitoring-stack-prometheus-server.monitoring.svc.cluster.local:80/api/v1/write
        tls:
          insecure: true
      
      debug:
        verbosity: detailed
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlp/tempo, debug]
        
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch]
          exporters: [prometheusremotewrite, debug]
        
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [debug]
  
  service:
    type: ClusterIP
